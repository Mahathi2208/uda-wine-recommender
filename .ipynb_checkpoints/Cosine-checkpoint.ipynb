{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import nltk, string, numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl \n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.read_csv('word_freq.csv')\n",
    "df = pd.read_csv('wine.csv')\n",
    "\n",
    "review_ls = df['reviews'].tolist()\n",
    "\n",
    "review_ls_clean = []\n",
    "\n",
    "for sentence in review_ls:\n",
    "    review = \"\"\n",
    "    for word in sentence:\n",
    "        word = re.sub(r'[^\\w]', ' ', word)\n",
    "        review += word \n",
    "    review_ls_clean.append(review)\n",
    "\n",
    "review_ls_clean\n",
    "\n",
    "two = []\n",
    "two.append(review_ls_clean[0])\n",
    "two.append(review_ls_clean[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/kevin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet') # first-time use only\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hard': 16, 'fault': 13, 'wine': 38, 'price': 27, 'vanilla': 36, 'oak': 23, 'hit': 18, 'followed': 14, 'red': 28, 'fruit': 15, 'hint': 17, 'pepper': 25, 's': 29, 'lot': 21, 'jam': 19, 'black': 1, 'cherry': 5, 'palate': 24, 'bit': 0, 'sweeter': 33, 'd': 7, 'prefer': 26, 'easy': 11, 'drink': 9, 'make': 22, 'easier': 10, 'treasure': 35, 'especially': 12, 'club': 6, 'card': 4, 'jammy': 20, 'second': 30, 'bottle': 3, 'week': 37, 'dark': 8, 'bold': 2, 'sumptuous': 32, 'sub': 31, 'tenner': 34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/Library/Python/3.9/lib/python/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "LemVectorizer = CountVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "LemVectorizer.fit_transform(two)\n",
    "\n",
    "print(LemVectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0 1 0 1 0 2 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 2 1 3 0 0 0 1 0 0\n",
      "  1 0 2]\n",
      " [1 0 1 3 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 2 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1\n",
      "  0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "tf_matrix = LemVectorizer.transform(two).toarray()\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.         1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.         1.40546511 1.\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfTran = TfidfTransformer(norm=\"l2\")\n",
    "tfidfTran.fit(tf_matrix)\n",
    "print(tfidfTran.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The idf for terms that appear in one document: 1.916290731874155\n",
      "The idf for terms that appear in two documents: 1.5108256237659907\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def idf(n,df):\n",
    "    result = math.log((n+1.0)/(df+1.0)) + 1\n",
    "    return result\n",
    "print (\"The idf for terms that appear in one document: \" + str(idf(4,1)))\n",
    "print (\"The idf for terms that appear in two documents: \" + str(idf(4,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11925967 0.16761531 0.         0.         0.         0.16761531\n",
      "  0.         0.16761531 0.         0.33523062 0.16761531 0.16761531\n",
      "  0.         0.16761531 0.16761531 0.11925967 0.16761531 0.16761531\n",
      "  0.16761531 0.16761531 0.         0.16761531 0.16761531 0.16761531\n",
      "  0.16761531 0.16761531 0.16761531 0.23851934 0.16761531 0.35777902\n",
      "  0.         0.         0.         0.16761531 0.         0.\n",
      "  0.16761531 0.         0.33523062]\n",
      " [0.13947127 0.         0.19602201 0.58806602 0.19602201 0.\n",
      "  0.19602201 0.         0.19602201 0.         0.         0.\n",
      "  0.19602201 0.         0.         0.13947127 0.         0.\n",
      "  0.         0.         0.39204401 0.         0.         0.\n",
      "  0.         0.         0.         0.13947127 0.         0.13947127\n",
      "  0.19602201 0.19602201 0.19602201 0.         0.19602201 0.19602201\n",
      "  0.         0.19602201 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
    "print (tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.11643309]\n",
      " [0.11643309 1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
    "print(cos_similarity_matrix)\n",
    "\n",
    "len(cos_similarity_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11643308762213182"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity(two)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
